```{r, echo=FALSE}
plotROC <- function (mu, sigma, FPF, TPF)
{
  zeta <- seq(- mu - 2,+ mu + 2,0.01)
  fpfArr <- array(dim = length(zeta))
  tpfArr <- array(dim = length(zeta))
  for (i in 1:length(zeta)) {
    fpfArr[i] <- pnorm(-zeta[i])
    tpfArr[i] <- pnorm((mu -zeta[i])/sigma) 
  }
  curveData <- data.frame(FPF = c(1, fpfArr, 0), TPF = c(1, tpfArr, 0))
  pointsData <- data.frame(FPF = FPF, TPF = TPF)
  rocPlot <- 
    ggplot(mapping = aes(x = FPF, y = TPF)) + 
    geom_line(data = curveData) + 
    geom_point(data = pointsData, shape = 1, size = 3)
  print(rocPlot)
}
```


## Threshold-independent performance measure {#binormal-model-scalar-measure}

A sensitivity-specificity pair is a two-valued measure of performance. Using such a measure it is difficult to unambiguously compare two systems. For example, if sensitivity is higher for one system but specificity is higher for another, this could be due to the first system using a lower threshold (since sensitivity and specificity are affected in opposite directions by a changing threshold). Desirable is a scalar (one-valued) measure of performance that does not depend on any specific threshold. 

Generally accepted measures are the partial-area $A_{z;c}$ under the ROC, Eqn. \@ref(eq:binormal-model-partial-area-final), the full-area $A_z$ under the ROC, Eqn. \@ref(eq:binormal-model-ab-2az), and the $d'$ index, Eqn. \@ref(eq:binormal-model-ab-2dprime). 

Before deriving analytical expressions for these measures let us further examine the premise that sensitivity-specificity is undesirable because it is a two-valued measure. A trivial way to convert a two-valued measure to a scalar measure is to sum the two values: high sensitivity and high specificity are both desirable, so a high value of their sum is certainly also desirable. In fact this is the basis for the Youden index, defined as sensitivity plus specificity minus one [@youden1950index] -- subtracting one makes the Youden index range from 0 to 1. However, this index varies with the position of the operating point on the ROC curve - from zero at the bottom-left corner of the ROC, rising to a maximum near the top-left, and falling to zero at the top-right. (The operating point at which it is maximum is sometimes regarded as the optimal operating point on the ROC curve. This issue is examined in detail in the chapter on optimal operating point in `RJafrocFrocBook`.)

To emphasize, we desire a scalar measure that is independent of threshold. This is approached in two steps in the following section.



### Full AUC {#binormal-model-partial-auc-recommendation}

Some researchers have recommended usage of the partial area measure on the grounds that it emphasizes the clinically relevant part of the ROC curve, by which they mean the initial steep portion, i.e., near the origin. I disagree. The partial area measure ignores all cases with z-samples smaller than that corresponding to $c$, i.e., $\Phi^{-1}(-c)$. This is a bad idea for several reasons. (1) There is an inevitable statistical power penalty that results from the reduced number of cases. (2) The z-samples corresponding to the steep portion of the curve are those of relatively easy cases (high z-samples).  


## Partial AUC vs. true performance {#binormal-model-partial-true}

* A *partial-area observer* such as in Section \@ref(binormal-model-partial-auc), rates cases as follows: for the sub-set of cases defined by $z \ge \zeta_1$ the observer reports *explicit* ratings exactly equal to the observed z-samples (or some monotonic transformation of the z-samples). For the remaining cases the observer assigns a *fixed value rating that is smaller than $\zeta_1$* (the exact value does not matter; these cases are said to be assigned *implicit* ratings). 

* In contrast, the *full-area observer* reports explicit ratings *for all cases*. 

* To measure true performance of the partial-area observer one must, of course, include all cases.

* The ROC curve extends continuously from the origin to the solid dot *plus the area under the dotted line* extending from the solid dot to (1,1). True performance, the area under the continuous section plus that under the straight line extension, is denoted $A_{z;c,TRUE}$ and is defined by:


\begin{equation} 
A_{z;c,\text{TRUE}} = A_{z;c} + \frac {\left ( 1 - FPF \right ) \left ( 1 + TPF \right )}{2}
(\#eq:binormal-model-partial-area-true-performance)
\end{equation}

In other words one adds to $A_{z;c}$ the area of the trapezoid with bases each equal to $(1 - FPF)$ and opposing sides equal to $TPF$ and unity. 

Since the partial-area observer does not preserve ordering information, *true performance of a partial-area observer is smaller than performance $A_z$ of a full-area observer*. 

\begin{equation} 
A_{z;c,\text{TRUE}} \le A_{z}
(\#eq:binormal-model-true-performance-az-inequality)
\end{equation}


True performance is illustrated with the following simulation 2AFC study. The `Wilcoxon` function, defined next, can be thought of as the mathematical equivalent of a 2AFC study, conducted with all possible pairings of non-diseased and diseased cases. For each pairing, if the z-sample of the diseased case exceeds that of the non-diseased case one adds unity to a zero-initialized counter; if it is smaller one does nothing; if they are equal one adds 0.5; and finally one divides by the number of comparisons.


```{r, echo=TRUE, attr.source = ".numberLines"}
Wilcoxon <- function (zk1, zk2)
{
  K1 = length(zk1)
  K2 = length(zk2)
  W <- 0
    for (k1 in 1:K1) {
      W <- W + sum(zk1[k1] < zk2)
      W <- W + 0.5 * sum(zk1[k1] == zk2)
    }
    W <- W/K1/K2
  return (W)
}
```



The following code saves 10,000 pairs of ratings in two arrays: `z[1,]` and `z[2,]`. The first array corresponds to non-diseased cases and the second to diseased cases. Note the usage, at lines 3-4, of the $a,b$ values to define the two distributions. The array `zc`, initially a copy of `z`, is selectively binned by setting, lines 6-7, all ratings less than $\zeta_1$ to -100. The ordering information for these z-samples is lost. 



<!-- ```{r, attr.source = ".numberLines"} -->
<!-- nPairs <- 10000 -->
<!-- z <- array(dim = c(2, nPairs)) -->
<!-- z[1,] <- rnorm(nPairs, sd = b) -->
<!-- z[2,] <- rnorm(nPairs, mean = a, sd = 1) -->
<!-- zc <- z -->
<!-- zc[1,z[1,] < zeta1] <- -100 # ratings of partial area observer -->
<!-- zc[2,z[2,] < zeta1] <- -100 # do: -->
<!-- ``` -->



The following code prints the predicted and observed full areas under the ROCs followed by the predicted and observed true performances. With this many cases sampling variability is small and the predicted and observed values are close. 



```{r, echo=FALSE}
cat("A_z predicted = ", 
    A_z, 
    "\nA_z observed = ", 
    Wilcoxon(z[1,], z[2,]),"\n")
cat("A_z{c;true} predicted = ", 
    A_zc + (1-opPtx)*(1+opPty)/2, 
    "\nA_z{c;true} observed = ", 
    Wilcoxon(zc[1,], zc[2,]),"\n")
```


Note that:

* $A_{z;c,\text{TRUE}} < A_z$, because ordering information is lost for all cases with z-samples less than $\zeta_1$. 
* $A_{z;c,\text{TRUE}} >> A_{z;c}$, because of the large contribution from the area under the straight line, left poanel Fig. \@ref(fig:binormal-model-threshold-dependence-2). 


## Illustrative plots {#binormal-model-illustrative-plots}

In the ROC plots below the partial-area observer curve is shown as a continuous line extending from the origin to the limiting point *plus* a dotted line extending from the limiting point to (1,1). The continuous section is determined by cumulating cases with z-samples $z \ge \zeta_1$ while the (1,1) point is determined by cumulating all cases. 


The ROC curve for both types of observers is shown in the left panel of \@ref(fig:binormal-model-threshold-dependence-2) for the following parameters: $a = 2$, $b = 1$ and $\zeta_1 = 1.5$; $\zeta_1$ corresponds to $c \equiv FPF =  \Phi(-\zeta_1)$ = `r pnorm(-zeta1)` and $TPF =  \Phi(a - b\zeta_1)$ = `r pnorm(a - b * zeta1)`. This point is shown in the plot by the solid dot. Partial AUC $A_{z;c}$ equals `r A_zc`. The full-area ROC curve, shown by the complete solid curve, extends from (0,0) to (1,1), the area under which is $A_z$ = `r A_z`. 




```{r echo=FALSE}
zeta1Arr <- seq(zeta1,5.5,0.05)
zeta1ArrAll <- seq(-3,5.5,0.05)
FPF <- pnorm(-zeta1Arr)
TPF <- pnorm(a - b*zeta1Arr)
continuous <- data.frame(FPF = pnorm(-zeta1ArrAll), TPF = pnorm(a - b*zeta1ArrAll))
dashed <- data.frame(FPF = c(FPF[1], 1), TPF = c(TPF[1], 1))
endPoint <- data.frame(x = FPF[1], y = TPF[1])
p1 <- ggplot2::ggplot(continuous, aes(x = FPF, y = TPF)) + 
  geom_line() + 
  geom_line(data = dashed, aes(x = FPF, y = TPF), linetype = 3) + 
  geom_point(data = endPoint, aes(x = x, y = y), size = 4) + 
  scale_x_continuous(limits = c(0,1)) + scale_y_continuous(limits = c(0,1))
```





```{r, echo=FALSE, cache=TRUE}
fpfArr <- c(seq(0.0 , 0.0625, 0.001), seq(0.0625, 1, 0.01)) # FPF array
tpfArr <- pnorm(a + b * qnorm(fpfArr)) # TPF array
rho <- -b/sqrt(1+b^2)
Lower1 <- -Inf
Upper1 <- qnorm(fpfArr)
Lower2 <- -Inf
Upper2 <- a/sqrt(1+b^2)
sigma <- rbind(c(1, rho), c(rho, 1))
pAucTrue <- array(dim = length(fpfArr))
for (i in 1:length(fpfArr)) {
  pAucTrue[i] <- as.numeric(pmvnorm(c(Lower1, Lower2), c(Upper1[i], Upper2), sigma = sigma))
  # add area under dashed line
  pAucTrue[i] <- pAucTrue[i] + (1 - fpfArr[i])*(1-tpfArr[i])/2 + (1-fpfArr[i]) * tpfArr[i]
}
df <- data.frame(FPF = fpfArr, pAUC_TRUE = pAucTrue)
p2 <- ggplot2::ggplot(data = df, mapping = aes(x = FPF, y = pAUC_TRUE)) + 
  geom_line() + 
  scale_x_continuous(limits = c(0, 1)) + 
  scale_y_continuous(limits = c(0.5,1))
```




```{r binormal-model-threshold-dependence-2, fig.cap="Left panel: binormal ROC curve corresponding to a = 2 and b = 1. The dot is the operating point corresponding to $\\zeta_1 = 1.5$. The continuous curve extending from the origin to (1,1) represents the full ROC. Note that in the region above the dot the continuus curve is above the dotted line, meaning true performance of an observer who only rates a sub-set of cases is less than performance of an observer who rates all cases. Right panel: variation of true performance with FPF; at FPF = 0 the plot starts at ordinate equal to 0.5 and levels out at FPF = 1 at AUC = $A_z = 0.921.$", fig.show='hold', echo=FALSE}
grid.arrange(p1, p2, ncol = 2)
```


The right panel of Fig. \@ref(fig:binormal-model-threshold-dependence-2) shows the variation of true performance $A_{z;c,\text{TRUE}}$ with FPF. As FPF increases true-performance increases. The curve starts from (0, 0.5) and ends at `r OpPtStr(1, max(pAucTrue))`. For low values of FPF the curve is very steep while for FPF > 0.25 the curve levels out, approaching the maximum value defined by $A_z$ = `r A_z`. True performance is maximized at $\zeta_1 = -\infty$.



Fig. \@ref(fig:binormal-model-threshold-dependence-3), left panel, corresponding to $a = 1$, $b = 0.2$ and $\zeta_1 = 1.5$, shows an improper ROC curve. The dashed line is well above the continuous curve and true performance is maximized at a finite value of $\zeta_1$, corresponding to $FPF =  0.153$, see right panel. This is an invalid conclusion since an improper ROC curve is a fitting artifact of the binormal model easily avoided by using modern curve-fitting methods (eg., PROPROC, CBM or RSM). TBA However, since the wAFROC has an operating characteristic with an improper-like feature but which is not a fitting artifact, this example serves a purpose, elaborated on in TBA Chapter (optim-op-point), where it is shown that by maximizing the area under the wAFROC one can find the optimal threshold of an algorithmic observer.


```{r echo=FALSE}
a <- 1; b <- 0.2
zeta1Arr <- seq(1.5,5.5,0.05)
zeta1ArrAll <- seq(-10,15,0.05)
xArr <- pnorm(-zeta1Arr)
yArr <- pnorm(a - b*zeta1Arr)
continuous <- data.frame(xArr = pnorm(-zeta1ArrAll), yArr = pnorm(a - b*zeta1ArrAll))
dashed <- data.frame(xArr = c(xArr[1], 1), yArr = c(yArr[1], 1))
endPoint <- data.frame(x = xArr[1], y = yArr[1])
p1 <- ggplot2::ggplot(continuous, aes(x = xArr, y = yArr)) + 
  geom_line() + 
  geom_line(data = dashed, aes(x = xArr, y = yArr), linetype = 3) + 
  geom_point(data = endPoint, aes(x = x, y = y), size = 4) + 
  scale_x_continuous(limits = c(0,1)) + scale_y_continuous(limits = c(0,1))
```




```{r, echo=FALSE, cache=FALSE}
A_z <- pnorm(a/sqrt(1+b^2))
fpfArr <- c(seq(0.0 , 0.0625, 0.001), seq(0.0625, 1, 0.01)) # FPF array
tpfArr <- pnorm(a + b * qnorm(fpfArr)) # TPF array
rho <- -b/sqrt(1+b^2)
Lower1 <- -Inf
Upper1 <- qnorm(fpfArr)
Lower2 <- -Inf
Upper2 <- a/sqrt(1+b^2)
sigma <- rbind(c(1, rho), c(rho, 1))
pAucTrue <- array(dim = length(fpfArr))
for (i in 1:length(fpfArr)) {
  pAucTrue[i] <- as.numeric(pmvnorm(c(Lower1, Lower2), c(Upper1[i], Upper2), sigma = sigma))
  # add area under dashed line
  pAucTrue[i] <- pAucTrue[i] + (1 - fpfArr[i])*(1-tpfArr[i])/2 + (1-fpfArr[i]) * tpfArr[i]
}
df <- data.frame(FPF = fpfArr, pAUC_TRUE = pAucTrue)
p2 <- ggplot2::ggplot(data = df, mapping = aes(x = FPF, y = pAUC_TRUE)) + 
  geom_line() + 
  scale_x_continuous(limits = c(0, 1)) + 
  scale_y_continuous(limits = c(0.5,1))
cat("true performance max occurs at FPF = ", fpfArr[which (pAucTrue == max(pAucTrue))], "\n")
```


```{r binormal-model-threshold-dependence-3, fig.cap="The left panel shows the visibly improper ROC curve for a = 1 and b = 0.2. The solid line is below the dotted line. The right panel shows the variation of true performance pAUC_TRUE with FPF. True performance is maximized at FPF = 0.153. Since improper ROC fits are fitting artifacts, this example does not negate the previous finding that true performance for a proper ROC curve is maximized by setting the threshold to report all cases, i.e., FPF = 1.", fig.show='hold', echo=FALSE}
grid.arrange(p1, p2, ncol = 2)
```


## Geometrical argument {#binormal-model-geometrical-argument}

Defining geometrical features of a proper ROC are: 

* As one moves up the curve the slope decreases monotonically; 
* At each point the slope is greater than that of the straight line connecting the point to (1,1); 
* The curve ends at (1,1). 

The geometry ensures that true performance for a proper ROC is maximized at $\zeta_1 = -\infty$, i.e., at FPF = 1, as in Fig. \@ref(fig:binormal-model-threshold-dependence-2), right panel.


## Optimal operating point on ROC {#binormal-model-optimal-op-pt} 

We have seen that optimal ROC AUC is achieved by setting $\zeta_1 = -\infty$, i.e., by reporting all cases as diseased. Of course, from clinical considerations, this is nonsense. Consider screening mammography, where typically for every 1000 cases only 5 are malignant. Recalling everybody would incur huge costs from having to rule out cancer in 995 actually non-diseased patients. Of course the 5 malignant cancers would be confirmed at the follow-up diagnostic mammography examination. But one can clearly see that the benefit of correctly detecting the 5 malignancies is far outweighed by the 995 unnecessary recalls. And if one is going to recall everybody, why perform the initial screening mammography exam?

So what is going on? The problem is that AUC measures classification performance in a 2AFC task. A screening examination is not a 2AFC task: the radiologist is not presented two cases, one non-diseased and one diseased, and asked to pick the diseased patient. Rather, the radiologist is shown images of a single patient, and the object is to maximize the detection rate for an acceptable number of false positives. 

To address this optimization task one needs to know the costs and benefits of the four decision outcomes in the binary paradigm: true and false positives, and true and false negatives. This has been addressed in [@metz1978rocmethodology]. Here is the reasonaing. Let 

* $C_0$ denote the overhead cost of performing the imaging examination, 
* $C_{\text{TP}}$ denote the cost of a true positive decision (a benefit can be expressed as a negative cost), 
* $C_{\text{FN}}$ denote the cost of a false negative decision,
* $C_{\text{FP}}$ denote the cost of a false positive decision, and 
* $C_{\text{TN}}$ denote the cost (or negative benefit) of a true negative decision.    


It is shown [@metz1978rocmethodology] that the average cost of the examination is:


\begin{equation} 
\overline{C} = C_0 + C_{\text{TP}} P(\text{TP}) 
+ C_{\text{TN}} P(\text{TN})
+ C_{\text{FP}} P(\text{FP})
+ C_{\text{FN}} P(\text{FN})
(\#eq:binormal-model-cost)
\end{equation}

In this equation $P(\text{TP})$ is the probability of a TP-event, etc. These probabilities are related to disease prevalence $P(+)$ and the operating point by:

\begin{equation} 
\left.
\begin{aligned}
P(\text{TP}) =& P(+) y \\
P(\text{TN}) =& (1-P(+)) (1-\text{FPF}) \\
P(\text{FP}) =& (1-P(+)) \text{FPF} \\
P(\text{FN}) =& P(+) (1-\text{TPF}) 
\end{aligned}
\right \}
(\#eq:binormal-model-cost2)
\end{equation}


With these substitutions one gets for the average cost:


\begin{equation} 
\overline{C} = C_0 + C_{\text{TN}} P(-) + C_{\text{FN}}  P(+) \\
+ (C_{\text{TP}} - C_{\text{FN}}) P(+) \text{TPF} \\
+ (C_{\text{FP}} - C_{\text{TN}}) P(-) \text{FPF} \\
(\#eq:binormal-model-cost3)
\end{equation}

Equating the derivative of the average cost to zero, to minimize the average cost, one gets:


\begin{equation} 
\frac{d(\text{TPF})}{d(\text{FPF})} = \frac{C_{\text{FP}} - C_{\text{TN}}}{C_{\text{FN}} - C_{\text{TP}}}\frac{P(-)}{P(+)}
(\#eq:binormal-model-cost4)
\end{equation}

This defines the slope $\frac{d(\text{TPF})}{d(\text{FPF})}$ of the ROC at the optimal operating point, i.e., the point that minimizes the average cost of the examination. Note that $P(-) = 1 - P(+)$. 

* If disease prevalence is high, then the optimal operating point is where the slope of the ROC is low, which is near the  upper-right corner. With mostly diseased cases it makes sense to set the operating point at high sensitivity and low specificity. Conversely, with low prevalence, one should set the operating point at low sensitivity and high specificity.

* For a given disease prevalence, if the cost of a FP decision is high (or if the benefit of a TN is high - recall that a benefit is the same as a negative cost), then the optimal operating point is where the slope of the ROC is high, which is near the lower-left corner. One sets the operating point at low sensitivity and high specificity.

* For a given disease prevalence, if the cost of a FN decision is high (or if the benefit of a TP is high), then the optimal operating point is where the slope of the ROC is low, which is near the  upper-right corner. One sets the operating point at high sensitivity and low specificity.


The costs and benefits are often difficult to quantify. If one assumes that the right hand side of Eqn. \@ref(eq:binormal-model-cost4) equals unity (e.g., the four costs / benefits are equal and disease-prevalence is 50%) then the optimal operating point is defined by that point on the ROC curve where the slope is unity, which is the point of nearest approach of the curve to the upper-left corner. This corresponds to maximizing the Youden index [@youden1950index], defined as the sum of sensitivity and specificity minus one. This is demonstrated in the following code.

```{r}
a <- 2;b <- 1
z <- seq(-3,5.5,0.05)
FPF <- pnorm(-z)
TPF <- pnorm(a - b*z)
Youden <- TPF + (1 - FPF) - 1
curve <- data.frame(FPF = FPF, TPF = TPF, YOU = Youden)
dist <- sqrt(FPF^2 + (1 - TPF)^2)
p1 <- ggplot2::ggplot(curve, aes(x = FPF, y = TPF)) + 
  geom_line() +
  scale_x_continuous(limits = c(0,1)) + scale_y_continuous(limits = c(0,1))
p2 <- ggplot2::ggplot(curve, aes(x = FPF, y = YOU)) + 
  geom_line() +
  scale_x_continuous(limits = c(0,1)) + scale_y_continuous(limits = c(0,1))
indxDist <- which(dist == min(dist))
indxYoud <- which(Youden == max(Youden))
if (indxDist != indxYoud) stop("The two indices are different") else {
  cat("Op Pt corresponding to max Youden and min distance is: \nFPF = ", 
      FPF[indxDist], 
      "\nTPF = ", 
      TPF[indxDist])
}
```


```{r binormal-model-youden-max, fig.cap="Left panel: binormal ROC curve corresponding to a = 2 and b = 1. Right panel: variation of Youden index with FPF; the plot shows a maximum at FPF = 0.1586553; this corresponds to the nearest approch of the ROC curve to the upper-left corner.", fig.show='hold', echo=FALSE}
grid.arrange(p1, p2, ncol = 2)
```


